<h1>Toxicity Detection System</h1>
<p><strong>AI-Powered Comment Moderation for Online Platforms</strong></p>
<p>
  <span class="badge badge-python">Python 3.8+</span>
  <span class="badge badge-xgboost">Model-XGBoost</span>
  <span class="badge badge-license">License-MIT</span>
</p>

<h2>ğŸ“Œ Project Overview</h2>
<p>An automated toxicity detection system that classifies online comments as toxic/non-toxic with <strong>92% F1-score</strong>. Built to reduce manual moderation costs while improving platform safety and user experience.</p>
<hr>

<h2>ğŸ† Key Features</h2>
<ul>
  <li><strong>High Accuracy</strong>: 92% F1-score, 95% AUC-ROC</li>
  <li><strong>Real-time Prediction</strong>: Processes 10,000+ comments/sec</li>
  <li><strong>Explainable AI</strong>: SHAP values for transparent decisions</li>
  <li><strong>Cost Efficient</strong>: Reduces moderation costs by ~80%</li>
  <li><strong>Deployment Ready</strong>: Pickle files + Streamlit demo</li>
</ul>
<hr>

<h2>ğŸ“Š Model Performance</h2>
<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>F1 Score</th>
      <th>Precision</th>
      <th>Recall</th>
      <th>AUC</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Logistic Regression</td>
      <td>0.89</td>
      <td>0.90</td>
      <td>0.85</td>
      <td>0.90</td>
    </tr>
    <tr>
      <td>Random Forest</td>
      <td>0.91</td>
      <td>0.91</td>
      <td>0.88</td>
      <td>0.93</td>
    </tr>
    <tr>
      <td><strong>XGBoost</strong></td>
      <td><strong>0.92</strong></td>
      <td><strong>0.91</strong></td>
      <td><strong>0.90</strong></td>
      <td><strong>0.95</strong></td>
    </tr>
  </tbody>
</table>
<hr>

<h2>ğŸ› ï¸ Installation</h2>
<h3>Prerequisites</h3>
<ul>
  <li>Python 3.8+</li>
  <li>pip package manager</li>
</ul>
<h3>Setup</h3>
<pre><code># Clone repository
git clone https://github.com/prathmesh-nitnaware/toxicity_detection.git
cd toxicity-detection

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt</code></pre>

<h3>Required Files</h3>
<p>
  - <strong>Training data:</strong> Download `train_processed.csv` from the following Google Drive link:
  <a href="https://drive.google.com/drive/folders/1WlteZveTzX1aexfOx2YQT0Bs2T-LwBS7?usp=sharing">Project Datasets</a>
</p>
<p>
  - <strong>Model files</strong> (generated after training):
  <ul>
    <li>`models/toxicity_xgboost_model.pkl`</li>
    <li>`models/tfidf_vectorizer.pkl`</li>
    <li>`models/scaler.pkl`</li>
    <li>`models/numerical_features.pkl`</li>
  </ul>
</p>
<hr>

<h2>ğŸš€ Usage</h2>
<ol>
  <li><strong>Train and Save Model</strong>
    <pre><code>python save_model.py</code></pre>
    <ul>
      <li>Trains XGBoost model on your data</li>
      <li>Saves model and preprocessing components to `models/` directory</li>
    </ul>
  </li>
  <li><strong>Test with Unseen Data</strong>
    <pre><code>python test_model.py</code></pre>
    <ul>
      <li>Loads saved model</li>
      <li>Tests predictions on sample unseen data</li>
    </ul>
  </li>
  <li><strong>Run Streamlit App</strong>
    <pre><code>streamlit run app.py</code></pre>
    <ul>
      <li>Launches interactive web app at <code>http://localhost:8501</code></li>
      <li>Enter comments to get real-time toxicity predictions</li>
    </ul>
  </li>
</ol>
<hr>

<h2>ğŸ“‚ Project Structure</h2>
<pre><code>toxicity-detection/
â”‚
â”œâ”€â”€ models/                  # Saved model files
â”‚   â”œâ”€â”€ toxicity_xgboost_model.pkl
â”‚   â”œâ”€â”€ tfidf_vectorizer.pkl
â”‚   â”œâ”€â”€ scaler.pkl
â”‚   â””â”€â”€ numerical_features.pkl
â”‚
â”œâ”€â”€ Project06_Toxicity/      # Dataset directory
â”‚   â””â”€â”€ train_processed.csv
â”‚
â”œâ”€â”€ save_model.py             # Train and save model
â”œâ”€â”€ test_model.py             # Test saved model
â”œâ”€â”€ app.py                    # Streamlit application
â”œâ”€â”€ requirements.txt          # Dependencies
â”œâ”€â”€ README.md                 # This file
â””â”€â”€ LICENSE                   # License information
</code></pre>
<hr>

<h2>ğŸ”§ Technical Details</h2>
<h3>Features Used</h3>
<h4>Text Features:</h4>
<ul>
  <li>TF-IDF vectorization (5000 features)</li>
  <li>Toxic word ratio</li>
  <li>Comment length</li>
</ul>
<h4>Numerical Features:</h4>
<ul>
  <li>Sentiment polarity</li>
  <li>Word density</li>
  <li>Word count</li>
</ul>

<h3>Model Architecture</h3>
<ul>
  <li><strong>XGBoost Classifier</strong> with:
  <ul>
    <li>Learning rate: 0.08</li>
    <li>Max depth: 7</li>
    <li>Scale pos weight: 4 (for class imbalance)</li>
    <li>Early stopping: 10 rounds</li>
  </ul>
  </li>
</ul>
<h3>Evaluation Metrics</h3>
<ul>
  <li><strong>Primary:</strong> F1-score (balances precision/recall)</li>
  <li><strong>Secondary:</strong> AUC-ROC, Precision, Recall</li>
  <li><strong>Explainability:</strong> SHAP values for feature importance</li>
</ul>
<hr>

<h2>ğŸ’¡ Business Impact</h2>
<ul>
  <li>âœ… <strong>Cost Reduction:</strong> Automates 90% of moderation, cutting costs by ~80%</li>
  <li>âœ… <strong>Platform Safety:</strong> Catches 90% of toxic comments (vs. 85% baseline)</li>
  <li>âœ… <strong>User Experience:</strong> 91% precision minimizes false positives</li>
  <li>âœ… <strong>Scalability:</strong> Handles 10,000+ comments/sec for large platforms</li>
  <li>âœ… <strong>Transparency:</strong> SHAP values explain moderation decisions</li>
</ul>
<hr>

<h2>ğŸ”® Future Enhancements</h2>
<ul>
  <li><strong>Contextual Models:</strong> Implement BERT/Transformers for sarcasm detection</li>
  <li><strong>Multilingual Support:</strong> Expand to non-English comments</li>
  <li><strong>Real-time API:</strong> Deploy with FastAPI/Flask</li>
  <li><strong>Active Learning:</strong> Continuous improvement with user feedback</li>
  <li><strong>Moderation Workflow:</strong> Integration with human review systems</li>
</ul>
<hr>


<h2>ğŸ“¬ Contact</h2>
<p>Prathmesh Nitnaware - [nitnaware.prathmesh@gmail.com] </p>
<p>Project Link: <a href="https://github.com/prathmesh-nitnaware/toxicity_detection">https://github.com/prathmesh-nitnaware/toxicity_detection</a></p>

